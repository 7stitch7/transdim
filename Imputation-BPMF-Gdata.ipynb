{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from numpy.random import multivariate_normal\n",
    "from scipy.stats import wishart\n",
    "\n",
    "def Normal_Wishart(mu_0, lamb, W, nu, seed=None):\n",
    "    \"\"\"Function drawing a Gaussian-Wishart random variable\"\"\"\n",
    "    Lambda = wishart(df=nu, scale=W, seed=seed).rvs()\n",
    "    cov = np.linalg.inv(lamb * Lambda)\n",
    "    mu = multivariate_normal(mu_0, cov)\n",
    "    return mu, Lambda\n",
    "\n",
    "def kr_prod(a, b):\n",
    "    return np.einsum('ir, jr -> ijr', a, b).reshape(a.shape[0] * b.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Computation Concepts\n",
    "\n",
    "## Kronecker product\n",
    "\n",
    "- **Definition**:\n",
    "\n",
    "Given two matrices $A\\in\\mathbb{R}^{m_1\\times n_1}$ and $B\\in\\mathbb{R}^{m_2\\times n_2}$, then, the **Kronecker product** between these two matrices is defined as\n",
    "\n",
    "$$A\\otimes B=\\left[ \\begin{array}{cccc} a_{11}B & a_{12}B & \\cdots & a_{1m_2}B \\\\ a_{21}B & a_{22}B & \\cdots & a_{2m_2}B \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m_11}B & a_{m_12}B & \\cdots & a_{m_1m_2}B \\\\ \\end{array} \\right]$$\n",
    "where the symbol $\\otimes$ denotes Kronecker product, and the size of resulted $A\\otimes B$ is $(m_1m_2)\\times (n_1n_2)$ (i.e., $m_1\\times m_2$ columns and $n_1\\times n_2$ rows).\n",
    "\n",
    "- **Example**:\n",
    "\n",
    "If $A=\\left[ \\begin{array}{cc} 1 & 2 \\\\ 3 & 4 \\\\ \\end{array} \\right]$ and $B=\\left[ \\begin{array}{ccc} 5 & 6 & 7\\\\ 8 & 9 & 10 \\\\ \\end{array} \\right]$, then, we have\n",
    "\n",
    "$$A\\otimes B=\\left[ \\begin{array}{cc} 1\\times \\left[ \\begin{array}{ccc} 5 & 6 & 7\\\\ 8 & 9 & 10\\\\ \\end{array} \\right] & 2\\times \\left[ \\begin{array}{ccc} 5 & 6 & 7\\\\ 8 & 9 & 10\\\\ \\end{array} \\right] \\\\ 3\\times \\left[ \\begin{array}{ccc} 5 & 6 & 7\\\\ 8 & 9 & 10\\\\ \\end{array} \\right] & 4\\times \\left[ \\begin{array}{ccc} 5 & 6 & 7\\\\ 8 & 9 & 10\\\\ \\end{array} \\right] \\\\ \\end{array} \\right]$$\n",
    "\n",
    "$$=\\left[ \\begin{array}{cccccc} 5 & 6 & 7 & 10 & 12 & 14 \\\\ 8 & 9 & 10 & 16 & 18 & 20 \\\\ 15 & 18 & 21 & 20 & 24 & 28 \\\\ 24 & 27 & 30 & 32 & 36 & 40 \\\\ \\end{array} \\right]\\in\\mathbb{R}^{4\\times 6}.$$\n",
    "\n",
    "## Khatri-Rao product (`kr_prod`)\n",
    "\n",
    "- **Definition**:\n",
    "\n",
    "Given two matrices $A=\\left( \\boldsymbol{a}_1,\\boldsymbol{a}_2,...,\\boldsymbol{a}_r \\right)\\in\\mathbb{R}^{m\\times r}$ and $B=\\left( \\boldsymbol{b}_1,\\boldsymbol{b}_2,...,\\boldsymbol{b}_r \\right)\\in\\mathbb{R}^{n\\times r}$ with same number of columns, then, the **Khatri-Rao product** (or **column-wise Kronecker product**) between $A$ and $B$ is given as follows,\n",
    "\n",
    "$$A\\odot B=\\left( \\boldsymbol{a}_1\\otimes \\boldsymbol{b}_1,\\boldsymbol{a}_2\\otimes \\boldsymbol{b}_2,...,\\boldsymbol{a}_r\\otimes \\boldsymbol{b}_r \\right)\\in\\mathbb{R}^{(mn)\\times r}$$\n",
    "where the symbol $\\odot$ denotes Khatri-Rao product, and $\\otimes$ denotes Kronecker product.\n",
    "\n",
    "- **Example**:\n",
    "\n",
    "If $A=\\left[ \\begin{array}{cc} 1 & 2 \\\\ 3 & 4 \\\\ \\end{array} \\right]=\\left( \\boldsymbol{a}_1,\\boldsymbol{a}_2 \\right) $ and $B=\\left[ \\begin{array}{cc} 5 & 6 \\\\ 7 & 8 \\\\ 9 & 10 \\\\ \\end{array} \\right]=\\left( \\boldsymbol{b}_1,\\boldsymbol{b}_2 \\right) $, then, we have\n",
    "\n",
    "$$A\\odot B=\\left( \\boldsymbol{a}_1\\otimes \\boldsymbol{b}_1,\\boldsymbol{a}_2\\otimes \\boldsymbol{b}_2 \\right) $$\n",
    "\n",
    "$$=\\left[ \\begin{array}{cc} \\left[ \\begin{array}{c} 1 \\\\ 3 \\\\ \\end{array} \\right]\\otimes \\left[ \\begin{array}{c} 5 \\\\ 7 \\\\ 9 \\\\ \\end{array} \\right] & \\left[ \\begin{array}{c} 2 \\\\ 4 \\\\ \\end{array} \\right]\\otimes \\left[ \\begin{array}{c} 6 \\\\ 8 \\\\ 10 \\\\ \\end{array} \\right] \\\\ \\end{array} \\right]$$\n",
    "\n",
    "$$=\\left[ \\begin{array}{cc} 5 & 12 \\\\ 7 & 16 \\\\ 9 & 20 \\\\ 15 & 24 \\\\ 21 & 32 \\\\ 27 & 40 \\\\ \\end{array} \\right]\\in\\mathbb{R}^{6\\times 2}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5 12]\n",
      " [ 7 16]\n",
      " [ 9 20]\n",
      " [15 24]\n",
      " [21 32]\n",
      " [27 40]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8], [9, 10]])\n",
    "print(kr_prod(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Probabilistic Matrix Factorization (BPMF)\n",
    "\n",
    "## Part 1: Model Description\n",
    "\n",
    "#### Gaussian assumption\n",
    "\n",
    "Given a matrix $Y\\in\\mathbb{R}^{m\\times f}$ which suffers from missing values, then the factorization can be applied to reconstruct the missing values within $Y$ by\n",
    "\n",
    "$$y_{it}\\sim\\mathcal{N}\\left(\\boldsymbol{w}_{i}^T\\boldsymbol{x}_{t},\\tau^{-1}\\right),\\forall (i,t),$$\n",
    "where $\\boldsymbol{w}_{i},\\boldsymbol{x}_{t}\\in\\mathbb{R}^{r}$ are latent factors, and the precision term $\\tau$ is an inverse of Gaussian variance.\n",
    "\n",
    "#### Bayesian framework\n",
    "\n",
    "Based on the Gaussian assumption over matrix elements $y_{it},(i,t)\\in\\Omega$ (where $\\Omega$ is a index set indicating observed matrix elements), the conjugate priors of model parameters (i.e., latent factors and precision term) and hyperparameters are given as\n",
    "\n",
    "$$\\boldsymbol{w}_{i}\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_{w},\\Lambda_{w}^{-1}\\right),\\forall i,$$\n",
    "$$\\boldsymbol{x}_{t}\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_{x},\\Lambda_{x}^{-1}\\right),\\forall j,$$\n",
    "$$\\tau\\sim\\text{Gamma}\\left(a_0,b_0\\right),$$\n",
    "$$\\boldsymbol{\\mu}_{w}\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_0,\\left(\\beta_0\\Lambda_w\\right)^{-1}\\right),\\Lambda_w\\sim\\mathcal{W}\\left(W_0,\\nu_0\\right),$$\n",
    "$$\\boldsymbol{\\mu}_{x}\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_0,\\left(\\beta_0\\Lambda_x\\right)^{-1}\\right),\\Lambda_x\\sim\\mathcal{W}\\left(W_0,\\nu_0\\right).$$\n",
    "\n",
    "## Part 2: Posterior Inference\n",
    "\n",
    "In the following, we will apply Gibbs sampling to implement our Bayesian inference for the matrix factorization task.\n",
    "\n",
    "#### - Sampling latent factors $\\boldsymbol{w}_{i},i\\in\\left\\{1,2,...,m\\right\\}$\n",
    "\n",
    "Draw $\\boldsymbol{w}_{i}\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_i^{*},(\\Lambda_{i}^{*})^{-1}\\right)$ with following parameters:\n",
    "\n",
    "$$\\boldsymbol{\\mu}_{i}^{*}=\\left(\\Lambda_{i}^{*}\\right)^{-1}\\left\\{\\tau\\sum_{t:(i,t)\\in\\Omega}y_{it}\\boldsymbol{x}_{t}+\\Lambda_w\\boldsymbol{\\mu}_w\\right\\},~\\Lambda_{i}^{*}=\\tau\\sum_{t:(i,t)\\in\\Omega}\\boldsymbol{x}_{t}\\boldsymbol{x}_{t}^{T}+\\Lambda_w.$$\n",
    "\n",
    "#### - Sampling latent factors $\\boldsymbol{x}_{t},t\\in\\left\\{1,2,...,f\\right\\}$\n",
    "\n",
    "Draw $\\boldsymbol{x}_{t}\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_t^{*},(\\Lambda_{t}^{*})^{-1}\\right)$ with following parameters:\n",
    "\n",
    "$$\\boldsymbol{\\mu}_{t}^{*}=\\left(\\Lambda_{t}^{*}\\right)^{-1}\\left\\{\\tau\\sum_{i:(i,t)\\in\\Omega}y_{it}\\boldsymbol{w}_{i}+\\Lambda_x\\boldsymbol{\\mu}_x\\right\\},~\\Lambda_{t}^{*}=\\tau\\sum_{i:(i,t)\\in\\Omega}\\boldsymbol{w}_{i}\\boldsymbol{w}_{i}^{T}+\\Lambda_x.$$\n",
    "\n",
    "#### - Sampling precision term $\\tau$\n",
    "\n",
    "Draw $\\tau\\in\\text{Gamma}\\left(a^{*},b^{*}\\right)$ with following parameters:\n",
    "\n",
    "$$a^{*}=a_0+\\frac{1}{2}|\\Omega|,~b^{*}=b_0+\\frac{1}{2}\\sum_{(i,j)\\in\\Omega}\\left(y_{it}-\\boldsymbol{w}_{i}^{T}\\boldsymbol{x}_{t}\\right)^2.$$\n",
    "\n",
    "#### - Sampling hyperparameters $\\left(\\boldsymbol{\\mu}_{w},\\Lambda_{w}\\right)$\n",
    "\n",
    "Draw\n",
    "\n",
    "- $\\Lambda_{w}\\sim\\mathcal{W}\\left(W_w^{*},\\nu_w^{*}\\right)$\n",
    "- $\\boldsymbol{\\mu}_{w}\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_{w}^{*},\\left(\\beta_w^{*}\\Lambda_w\\right)^{-1}\\right)$\n",
    "\n",
    "with following parameters:\n",
    "\n",
    "$$\\boldsymbol{\\mu}_{w}^{*}=\\frac{m\\boldsymbol{\\bar{w}}+\\beta_0\\boldsymbol{\\mu}_0}{m+\\beta_0},~\\beta_w^{*}=m+\\beta_0,~\\nu_w^{*}=m+\\nu_0,$$\n",
    "$$\\left(W_w^{*}\\right)^{-1}=W_0^{-1}+mS_w+\\frac{m\\beta_0}{m+\\beta_0}\\left(\\boldsymbol{\\bar{w}}-\\boldsymbol{\\mu}_0\\right)\\left(\\boldsymbol{\\bar{w}}-\\boldsymbol{\\mu}_0\\right)^T,$$\n",
    "where $\\boldsymbol{\\bar{w}}=\\sum_{i=1}^{m}\\boldsymbol{w}_{i},~S_w=\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\boldsymbol{w}_{i}-\\boldsymbol{\\bar{w}}\\right)\\left(\\boldsymbol{w}_{i}-\\boldsymbol{\\bar{w}}\\right)^T$.\n",
    "\n",
    "#### - Sampling hyperparameters $\\left(\\boldsymbol{\\mu}_{x},\\Lambda_{x}\\right)$\n",
    "\n",
    "Draw\n",
    "\n",
    "- $\\Lambda_{x}\\sim\\mathcal{W}\\left(W_x^{*},\\nu_x^{*}\\right)$\n",
    "- $\\boldsymbol{\\mu}_{x}\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_{x}^{*},\\left(\\beta_x^{*}\\Lambda_x\\right)^{-1}\\right)$\n",
    "\n",
    "with following parameters:\n",
    "\n",
    "$$\\boldsymbol{\\mu}_{x}^{*}=\\frac{f\\boldsymbol{\\bar{x}}+\\beta_0\\boldsymbol{\\mu}_0}{f+\\beta_0},~\\beta_x^{*}=f+\\beta_0,~\\nu_x^{*}=f+\\nu_0,$$\n",
    "$$\\left(W_x^{*}\\right)^{-1}=W_0^{-1}+fS_x+\\frac{f\\beta_0}{f+\\beta_0}\\left(\\boldsymbol{\\bar{x}}-\\boldsymbol{\\mu}_0\\right)\\left(\\boldsymbol{\\bar{x}}-\\boldsymbol{\\mu}_0\\right)^T,$$\n",
    "where $\\boldsymbol{\\bar{x}}=\\sum_{t=1}^{f}\\boldsymbol{x}_{t},~S_x=\\frac{1}{f}\\sum_{t=1}^{f}\\left(\\boldsymbol{x}_{t}-\\boldsymbol{\\bar{x}}\\right)\\left(\\boldsymbol{x}_{t}-\\boldsymbol{\\bar{x}}\\right)^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BPMF(dense_mat, sparse_mat, binary_mat, rank, maxiter1, maxiter2):\n",
    "    dim1 = sparse_mat.shape[0]\n",
    "    dim2 = sparse_mat.shape[1]\n",
    "    pos = np.where((dense_mat>0) & (sparse_mat==0))\n",
    "    position = np.where(sparse_mat > 0)\n",
    "    \n",
    "    W = 0.1 * np.random.randn(dim1, rank)\n",
    "    X = 0.1 * np.random.randn(dim2, rank)\n",
    "    \n",
    "    beta0 = 1\n",
    "    nu0 = rank\n",
    "    mu0 = np.zeros((rank))\n",
    "    tau = 1\n",
    "    a0 = 1\n",
    "    b0 = 1\n",
    "    W0 = np.eye(rank)\n",
    "    \n",
    "    for iter in range(maxiter1):\n",
    "        W_bar = np.mean(W, axis = 0)\n",
    "        var_mu0 = (dim1 * W_bar + beta0 * mu0)/(dim1 + beta0)\n",
    "        var_nu = dim1 + nu0\n",
    "        var_W = np.linalg.inv(np.linalg.inv(W0) \n",
    "                              + dim1 * np.cov(W.T) + dim1 * beta0/(dim1 + beta0)\n",
    "                             * np.outer(W_bar - mu0, W_bar - mu0))\n",
    "        var_W = (var_W + var_W.T)/2\n",
    "        var_mu0, var_Lambda0 = Normal_Wishart(var_mu0, dim1 + beta0, var_W, var_nu, seed = None)\n",
    "        \n",
    "        var1 = X.T\n",
    "        var2 = kr_prod(var1, var1)\n",
    "        var3 = tau * np.matmul(var2, binary_mat.T).reshape([rank, rank, dim1]) + np.dstack([var_Lambda0] * dim1)\n",
    "        var4 = tau * np.matmul(var1, sparse_mat.T) + np.dstack([np.matmul(var_Lambda0, var_mu0)] * dim1)[0, :, :]\n",
    "        for i in range(dim1):\n",
    "            var_Lambda1 = var3[ :, :, i]\n",
    "            inv_var_Lambda1 = np.linalg.inv((var_Lambda1 + var_Lambda1.T)/2)\n",
    "            var_mu = np.matmul(inv_var_Lambda1, var4[:, i])\n",
    "            W[i, :] = np.random.multivariate_normal(var_mu, inv_var_Lambda1)\n",
    "        \n",
    "        X_bar = np.mean(X, axis = 0)\n",
    "        var_mu0 = (dim2 * X_bar + beta0 * mu0)/(dim2 + beta0)\n",
    "        var_nu = dim2 + nu0\n",
    "        var_X = np.linalg.inv(np.linalg.inv(W0) \n",
    "                              + dim2 * np.cov(X.T) + dim2 * beta0/(dim2 + beta0)\n",
    "                             * np.outer(X_bar - mu0, X_bar - mu0))\n",
    "        var_X = (var_X + var_X.T)/2\n",
    "        var_mu0, var_Lambda0 = Normal_Wishart(var_mu0, dim2 + beta0, var_X, var_nu, seed = None)\n",
    "        \n",
    "        var1 = W.T\n",
    "        var2 = kr_prod(var1, var1)\n",
    "        var3 = tau * np.matmul(var2, binary_mat).reshape([rank, rank, dim2]) + np.dstack([var_Lambda0] * dim2)\n",
    "        var4 = tau * np.matmul(var1, sparse_mat) + np.dstack([np.matmul(var_Lambda0, var_mu0)] * dim2)[0, :, :]\n",
    "        for t in range(dim2):\n",
    "            var_Lambda1 = var3[ :, :, t]\n",
    "            inv_var_Lambda1 = np.linalg.inv((var_Lambda1 + var_Lambda1.T)/2)\n",
    "            var_mu = np.matmul(inv_var_Lambda1, var4[:, t])\n",
    "            X[t, :] = np.random.multivariate_normal(var_mu, inv_var_Lambda1)\n",
    "            \n",
    "        mat_hat = np.matmul(W, X.T)\n",
    "        rmse = np.sqrt(np.sum((dense_mat[pos] - mat_hat[pos])**2)/dense_mat[pos].shape[0])\n",
    "        \n",
    "        var_a = a0 + 0.5 * sparse_mat[position].shape[0]\n",
    "        error = sparse_mat - mat_hat\n",
    "        var_b = b0 + 0.5 * np.sum(error[position]**2)\n",
    "        tau = np.random.gamma(var_a, 1/var_b)\n",
    "        \n",
    "        if (iter + 1) % 100 == 0:\n",
    "            print('Iter: {}'.format(iter + 1))\n",
    "            print('RMSE: {:.6}'.format(rmse))\n",
    "            print()\n",
    "\n",
    "    W_plus = np.zeros((dim1, rank))\n",
    "    X_plus = np.zeros((dim2, rank))\n",
    "    mat_hat_plus = np.zeros((dim1, dim2))\n",
    "    for iters in range(maxiter2):\n",
    "        W_bar = np.mean(W, axis = 0)\n",
    "        var_mu0 = (dim1 * W_bar + beta0 * mu0)/(dim1 + beta0)\n",
    "        var_nu = dim1 + nu0\n",
    "        var_W = np.linalg.inv(np.linalg.inv(W0) \n",
    "                              + dim1 * np.cov(W.T) + dim1 * beta0/(dim1 + beta0)\n",
    "                             * np.outer(W_bar - mu0, W_bar - mu0))\n",
    "        var_W = (var_W + var_W.T)/2\n",
    "        var_mu0, var_Lambda0 = Normal_Wishart(var_mu0, dim1 + beta0, var_W, var_nu, seed = None)\n",
    "        \n",
    "        var1 = X.T\n",
    "        var2 = kr_prod(var1, var1)\n",
    "        var3 = tau * np.matmul(var2, binary_mat.T).reshape([rank, rank, dim1]) + np.dstack([var_Lambda0] * dim1)\n",
    "        var4 = tau * np.matmul(var1, sparse_mat.T) + np.dstack([np.matmul(var_Lambda0, var_mu0)] * dim1)[0, :, :]\n",
    "        for i in range(dim1):\n",
    "            var_Lambda1 = var3[ :, :, i]\n",
    "            inv_var_Lambda1 = np.linalg.inv((var_Lambda1 + var_Lambda1.T)/2)\n",
    "            var_mu = np.matmul(inv_var_Lambda1, var4[:, i])\n",
    "            W[i, :] = np.random.multivariate_normal(var_mu, inv_var_Lambda1)\n",
    "        W_plus += W\n",
    "        \n",
    "        X_bar = np.mean(X, axis = 0)\n",
    "        var_mu0 = (dim2 * X_bar + beta0 * mu0)/(dim2 + beta0)\n",
    "        var_nu = dim2 + nu0\n",
    "        var_X = np.linalg.inv(np.linalg.inv(W0) \n",
    "                              + dim2 * np.cov(X.T) + dim2 * beta0/(dim2 + beta0)\n",
    "                             * np.outer(X_bar - mu0, X_bar - mu0))\n",
    "        var_X = (var_X + var_X.T)/2\n",
    "        var_mu0, var_Lambda0 = Normal_Wishart(var_mu0, dim2 + beta0, var_X, var_nu, seed = None)\n",
    "        \n",
    "        var1 = W.T\n",
    "        var2 = kr_prod(var1, var1)\n",
    "        var3 = tau * np.matmul(var2, binary_mat).reshape([rank, rank, dim2]) + np.dstack([var_Lambda0] * dim2)\n",
    "        var4 = tau * np.matmul(var1, sparse_mat) + np.dstack([np.matmul(var_Lambda0, var_mu0)] * dim2)[0, :, :]\n",
    "        for t in range(dim2):\n",
    "            var_Lambda1 = var3[ :, :, t]\n",
    "            inv_var_Lambda1 = np.linalg.inv((var_Lambda1 + var_Lambda1.T)/2)\n",
    "            var_mu = np.matmul(inv_var_Lambda1, var4[:, t])\n",
    "            X[t, :] = np.random.multivariate_normal(var_mu, inv_var_Lambda1)\n",
    "        X_plus += X\n",
    "            \n",
    "        mat_hat = np.matmul(W, X.T)\n",
    "        mat_hat_plus += mat_hat\n",
    "        \n",
    "        var_a = a0 + 0.5 * sparse_mat[position].shape[0]\n",
    "        error = sparse_mat - mat_hat\n",
    "        var_b = b0 + 0.5 * np.sum(error[position]**2)\n",
    "        tau = np.random.gamma(var_a, 1/var_b)\n",
    "        \n",
    "    W = W_plus/maxiter2\n",
    "    X = X_plus/maxiter2\n",
    "    mat_hat = mat_hat_plus/maxiter2\n",
    "    final_mape = np.sum(np.abs(dense_mat[pos] - \n",
    "                               mat_hat[pos])/dense_mat[pos])/dense_mat[pos].shape[0]\n",
    "    final_rmse = np.sqrt(np.sum((dense_mat[pos] - \n",
    "                                 mat_hat[pos])**2)/dense_mat[pos].shape[0])\n",
    "    print('Final MAPE: {:.6}'.format(final_mape))\n",
    "    print('Final RMSE: {:.6}'.format(final_rmse))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Organization\n",
    "\n",
    "### Part 1: Matrix Structure\n",
    "\n",
    "We consider a dataset of $m$ discrete time series $\\boldsymbol{y}_{i}\\in\\mathbb{R}^{f},i=\\left\\{1,2,...,m\\right\\}$. The time series may have missing elements. We express spatio-temporal dataset as a matrix $Y\\in\\mathbb{R}^{m\\times f}$ with $m$ rows (e.g., locations) and $f$ columns (e.g., discrete time intervals),\n",
    "\n",
    "$$Y=\\left[ \\begin{array}{cccc} y_{11} & y_{12} & \\cdots & y_{1f} \\\\ y_{21} & y_{22} & \\cdots & y_{2f} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ y_{m1} & y_{m2} & \\cdots & y_{mf} \\\\ \\end{array} \\right]\\in\\mathbb{R}^{m\\times f}.$$\n",
    "\n",
    "### Part 2: Tensor Structure\n",
    "\n",
    "We consider a dataset of $m$ discrete time series $\\boldsymbol{y}_{i}\\in\\mathbb{R}^{nf},i=\\left\\{1,2,...,m\\right\\}$. The time series may have missing elements. We partition each time series into intervals of predifined length $f$. We express each partitioned time series as a matrix $Y_{i}$ with $n$ rows (e.g., days) and $f$ columns (e.g., discrete time intervals per day),\n",
    "\n",
    "$$Y_{i}=\\left[ \\begin{array}{cccc} y_{11} & y_{12} & \\cdots & y_{1f} \\\\ y_{21} & y_{22} & \\cdots & y_{2f} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ y_{n1} & y_{n2} & \\cdots & y_{nf} \\\\ \\end{array} \\right]\\in\\mathbb{R}^{n\\times f},i=1,2,...,m,$$\n",
    "\n",
    "therefore, the resulting structure is a tensor $\\mathcal{Y}\\in\\mathbb{R}^{m\\times n\\times f}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using numpy backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "from tensorly import unfold\n",
    "\n",
    "tensor = scipy.io.loadmat('Guangzhou-data-set/tensor.mat')\n",
    "tensor = tensor['tensor']\n",
    "random_matrix = scipy.io.loadmat('Guangzhou-data-set/random_matrix.mat')\n",
    "random_matrix = random_matrix['random_matrix']\n",
    "random_tensor = scipy.io.loadmat('Guangzhou-data-set/random_tensor.mat')\n",
    "random_tensor = random_tensor['random_tensor']\n",
    "\n",
    "dense_mat = unfold(tensor, 0)\n",
    "missing_rate = 0.2\n",
    "\n",
    "# =============================================================================\n",
    "### Random missing (RM) scenario:\n",
    "### ------------------------------\n",
    "###   missing rate | 0.2 | 0.4 |\n",
    "###   rank         |  80 |  80 |\n",
    "### ------------------------------\n",
    "### Set the RM scenario by:\n",
    "binary_mat = unfold(np.round(random_tensor + 0.5 - missing_rate), 0)\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "### Non-random missing (NM) scenario:\n",
    "### ------------------------------\n",
    "###   missing rate | 0.2 | 0.4 |\n",
    "###   rank         |  10 |  10 |\n",
    "### ------------------------------\n",
    "### Set the NM scenario by:\n",
    "# binary_tensor = np.zeros(tensor.shape)\n",
    "# for i1 in range(tensor.shape[0]):\n",
    "#     for i2 in range(tensor.shape[1]):\n",
    "#         binary_tensor[i1,i2,:] = np.round(random_matrix[i1,i2] + 0.5 - missing_rate)\n",
    "# binary_mat = unfold(binary_tensor, 0)\n",
    "# =============================================================================\n",
    "\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 100\n",
      "RMSE: 4.66602\n",
      "\n",
      "Iter: 200\n",
      "RMSE: 4.66057\n",
      "\n",
      "Iter: 300\n",
      "RMSE: 4.65929\n",
      "\n",
      "Iter: 400\n",
      "RMSE: 4.65814\n",
      "\n",
      "Iter: 500\n",
      "RMSE: 4.65388\n",
      "\n",
      "Iter: 600\n",
      "RMSE: 4.66249\n",
      "\n",
      "Iter: 700\n",
      "RMSE: 4.65891\n",
      "\n",
      "Iter: 800\n",
      "RMSE: 4.6603\n",
      "\n",
      "Iter: 900\n",
      "RMSE: 4.65752\n",
      "\n",
      "Iter: 1000\n",
      "RMSE: 4.66038\n",
      "\n",
      "Final MAPE: 0.0950871\n",
      "Final RMSE: 4.04034\n",
      "\n",
      "Running time: 47982 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "BPMF(dense_mat, sparse_mat, binary_mat, 80, 1000, 500)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment results** of missing data imputation using Bayesian probabilistic matrix factorization (BPMF):\n",
    "\n",
    "|  scenario |`rank`|`maxiter1`|`maxiter2`|       mape |      rmse |\n",
    "|:----------|-----:|---------:|---------:|-----------:|----------:|\n",
    "|**0.2, NM**|   10 |     1000 |      500 |     0.1054 | **4.3828**|\n",
    "|**0.4, NM**|   10 |     1000 |      500 |     0.1071 | **4.5586**|\n",
    "|**0.2, RM**|   80 |     1000 |      500 |     0.0951 | **4.0403**|\n",
    "|**0.4, RM**|   80 |     1000 |      500 |     0.0978 | **4.1578**|\n",
    "\n",
    "   > The experiment relies on the *Urban traffic speed data set in Guangzhou, China*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
